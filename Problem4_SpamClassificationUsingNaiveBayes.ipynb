{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification Using Naïve Bayes\n",
    "\n",
    "_Problem Instructions:_\n",
    "\n",
    "## a) Data Preparation\n",
    "Download the public TREC06 corpus and preprocess the text for spam filtering. You may use text processing tools to tokenize and clean the text, but all Naïve Bayes classification algorithms must be implemented from scratch. Consider removing common stopwords and using stemming/lemmatization to enhance text processing.\n",
    "\n",
    "## b) Dictionary Building and Feature Selection\n",
    "Build a dictionary for spam classification, limiting word selection to those with lengths between 3-20 alphabets. Discuss your approach to designing the filter, including feature selection and probability computation. How do you determine which words are most informative? Consider using term frequency-inverse document frequency (TF-IDF) as a metric for word importance.\n",
    "\n",
    "## c) Model Training and Evaluation\n",
    "Train and test your Naïve Bayes spam filter. Then, repeat the classification process using only the top 200 most informative words from the dictionary. Analyze the impact of this modification on classification performance. Does reducing the feature space improve or degrade performance? Justify your answer.\n",
    "\n",
    "## d) Handling Unseen Words\n",
    "Discuss how you handle words that appear in the test set but are not found in the training set. Suggest possible methods to mitigate issues arising from unseen words in real-world spam filtering applications. Consider Laplace (additive) smoothing and back-off models as potential solutions.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
